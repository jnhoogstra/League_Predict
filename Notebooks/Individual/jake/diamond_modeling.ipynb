{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solo Queue Modeling\n",
    "\n",
    "Using a dataset from LoL's Diamond I to Masters level soloq game stats at 10 minutes that Andre has engineered some features for, I begin my modeling process below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score, plot_confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# imports for visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pl\n",
    "import xgboost\n",
    "\n",
    "# I'm importing a functions.py file that I created so I can reuse a scoring function in the file\n",
    "import sys\n",
    "if not 'Notebooks/Individual/Jake' in sys.path:\n",
    "    sys.path.append('Notebooks/Individual/jake')\n",
    "from functions import ScoreModel, FeatureImp #featureimp is throwing errors about dfs not being defined\n",
    "\n",
    "# the below lines are to play an alert sound for when the notebook finishes running\n",
    "import IPython\n",
    "sound_file = '../../../archive/sounds/puzzle_solved_jingle.wav'\n",
    "\n",
    "# I wanted to be able to see all the columns when using .head()\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading in Andre's dataset and doing initial checks to make sure everything is there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../../archive/with_rates.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-7d5dd75c34ee>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"../../../archive/with_rates.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    684\u001b[0m     )\n\u001b[0;32m    685\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 686\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 452\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    453\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    945\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 946\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    947\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    948\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1176\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1177\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1178\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1179\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1180\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   2006\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2007\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2008\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2009\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2010\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../../archive/with_rates.csv'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"../../../archive/with_rates.csv\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking class balance\n",
    "df[\"blueWins\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Everything looks clear so its time to start modeling.  I've decided to use XGBClassifier for my modeling work, and I use gridsearch for hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_col = [\"blueWins\", \"gameId\", \"champions\", \"blueChamps\", \"redChamps\"]\n",
    "y = df[\"blueWins\"]\n",
    "X = df.drop(columns=drop_col, axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=57)\n",
    "\n",
    "boost_model = XGBClassifier(random_state=57, objective=\"reg:logistic\")\n",
    "\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'max_depth': [2, 3, 4, 5],\n",
    "    'min_child_weight': [1, 2, 3, 4, 5, 6],\n",
    "    'subsample': [0.4, 0.5, 0.6, 0.7],\n",
    "    'n_estimators': [30, 50, 100]\n",
    "}\n",
    "\n",
    "gridsearch = GridSearchCV(boost_model, param_grid, cv=3, scoring=\"accuracy\", n_jobs=1)\n",
    "gridsearch.fit(X_train, y_train)\n",
    "\n",
    "# now that the model is fit with gridsearch, I'm going to output the best parameters in case I need to use them for later\n",
    "best_parameters = gridsearch.best_params_\n",
    "print(\"Best Parameters: \")\n",
    "print(best_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBClassifier Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Scores\")\n",
    "print(ScoreModel(gridsearch, X_train, y_train))\n",
    "\n",
    "print(\"Test Scores\")\n",
    "print(ScoreModel(gridsearch, X_test, y_test))\n",
    "\n",
    "# as an aside, I have no idea how to get rid of the Nones below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our model is overfit and the only parameter that is at one of our limits in the param_grid is n_estimators.  Otherwise, a decent performance overall.\n",
    "\n",
    "#### After scoring, I was curious to see how a standard LogisticRegression model would perform in comparison to the gradient boosted model above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the scales below I used during testing, and applying the scaled to the logistic regression below actually made the model worse\n",
    "\n",
    "#ss = StandardScaler()\n",
    "#X_train_ss = ss.fit_transform(X_train)\n",
    "#X_test_ss = ss.transform(X_test)\n",
    "\n",
    "logreg = LogisticRegression().fit(X_train, y_train)\n",
    "\n",
    "print(\"Training Scores\")\n",
    "print(ScoreModel(logreg, X_train, y_train))\n",
    "\n",
    "print(\"Test Scores\")\n",
    "print(ScoreModel(logreg, X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The logistic regression performed better than I had anticipated, and is only ever so slightly overfit (0.7 difference in accuracy score)\n",
    "\n",
    "#### Next I want to analyze the feature importances for our XGBClassifier model\n",
    "\n",
    "Unfortunately GridSearchCV has no feature importances attribute, but that's why I output the best parameters earlier so I can make a temporary XGBC model with those parameters to get the feature importances that I'm looking for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning_rate': 0.05, 'max_depth': 3, 'min_child_weight': 5, 'n_estimators': 100, 'subsample': 0.6\n",
    "\n",
    "boost_model2 = XGBClassifier(random_state=57, objective=\"reg:logistic\",\n",
    "                            learning_rate= 0.05, max_depth = 3, min_child_weight = 5,\n",
    "                            n_estimators = 100, subsample = 0.6)\n",
    "\n",
    "boost_model2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i multiply the feature importances and round to make them easier read in our output\n",
    "features = list(zip(X_train.columns, 100*(np.round(boost_model2.feature_importances_, 4))))\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### So we've got some really low feature importances here.  I've decided to remove any features that were below 2 and their opposing team counterpart was also below 2.\n",
    "\n",
    "After removing those features I make a new XGBC model with gridsearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonimportant = [\"blueTowersDestroyed\", \"redTowersDestroyed\", \"blueWardsPlaced\", \"blueWardsDestroyed\",\n",
    "               \"blueKills\", \"blueAssists\", \"blueCSPerMin\", \"redWardsPlaced\", \"redWardsDestroyed\",\n",
    "               \"redKills\", \"redAssists\", \"redCSPerMin\"]\n",
    "\n",
    "X_non = X.drop(columns=nonimportant, axis=1)\n",
    "\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X_non, y, random_state=57)\n",
    "\n",
    "boost_model_non = XGBClassifier(random_state=57, objective=\"reg:logistic\")\n",
    "\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'max_depth': [2, 3, 4, 5],\n",
    "    'min_child_weight': [1, 2, 3, 4, 5, 6],\n",
    "    'subsample': [0.4, 0.5, 0.6, 0.7],\n",
    "    'n_estimators': [30, 50, 100]\n",
    "}\n",
    "\n",
    "gridsearch_non = GridSearchCV(boost_model, param_grid, cv=3, scoring=\"accuracy\", n_jobs=1)\n",
    "gridsearch_non.fit(X_train2, y_train2)\n",
    "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n",
    "best_parameters = gridsearch.best_params_\n",
    "\n",
    "print(\"Best Parameters: \")\n",
    "print(best_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Worth noting that our parameters here are fairly identical to the previous model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Scores\")\n",
    "print(ScoreModel(gridsearch_non, X_train2, y_train2))\n",
    "\n",
    "print(\"Test Scores\")\n",
    "print(ScoreModel(gridsearch_non, X_test2, y_test2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This model is much better than our first XGBC and is better overall than our Logistic Regression model too!  We're no longer overfit, and improved our scores accross the board.\n",
    "\n",
    "We're going to follow up with another feature importance inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boost_model_non = XGBClassifier(random_state=57, objective=\"reg:logistic\",\n",
    "                            learning_rate= 0.05, max_depth = 3, min_child_weight = 5,\n",
    "                            n_estimators = 100, subsample = 0.6)\n",
    "boost_model_non.fit(X_train2, y_train2)\n",
    "\n",
    "features = list(zip(X_train2.columns, 100*(np.round(boost_model_non.feature_importances_, 4))))\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling with new features \n",
    "\n",
    "Does either team have X impactful champion? (5 different champs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.read_csv(\"../../../archive/with_rates_and_spec_gods.csv\")\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_col = [\"blueWins\", \"gameId\", \"blueChamps\", \"redChamps\"]\n",
    "X_new = new_df.drop(drop_col, axis=1)\n",
    "X_new = X_new.drop(nonimportant, axis=1)\n",
    "y_new = new_df[\"blueWins\"]\n",
    "\n",
    "X_train_new, X_test_new, y_train_new, y_test_new = train_test_split(X_new, y_new, random_state=57)\n",
    "\n",
    "boost_model_new = XGBClassifier(random_state=57, objective=\"reg:logistic\",\n",
    "                            learning_rate= 0.05, max_depth = 3, min_child_weight = 5,\n",
    "                            n_estimators = 100, subsample = 0.6)\n",
    "boost_model_new.fit(X_train_new, y_train_new)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Training Scores\")\n",
    "print(ScoreModel(boost_model_new, X_train_new, y_train_new))\n",
    "\n",
    "print(\"Test Scores\")\n",
    "print(ScoreModel(boost_model_new, X_test_new, y_test_new))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here we have another decently performing model, however, we're still overfit to our training data.\n",
    "\n",
    "Let's check out our feature importances again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = list(zip(X_train_new.columns, 100*(np.round(boost_model_new.feature_importances_, 4))))\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Audio(sound_file, autoplay=True, rate=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "It seems that the first 10 minutes of a game are significantly impactful on the outcome of a match.  My models are consistently producing above a 70% accuracy score, with my best model at 74%.\n",
    "\n",
    "Even though the first 10 minutes are impactful, our results are only 24% better than random guessing (which would be 50%).  I attribute this to the fact that game time averages around 30 minutes, so there's still plenty of time for the disadvantaged team to make a comeback, and in some rare cases the winning team is still at a disadvantage in terms of overall statistics.  If we were take data from less skilled players, we could expect this variability in wins to be more apparent.\n",
    "\n",
    "As far as features that are driving our model, team gold and experience leads are the most important, with a couple features like dragons and the average winrate of a team's selected champions being less important, but not something that's necessarily correlated with gold and experience leads.  In the case of dragons, they provide team wide buffs that aren't measured in terms of gold or experience (though killing the dragon awards a small amount of gold and xp)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visuals for presentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost.plot_importance(boost_model_non)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
